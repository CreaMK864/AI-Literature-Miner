import torch
import cv2
import numpy as np
import albumentations as A
from albumentations.pytorch import ToTensorV2
import segmentation_models_pytorch as smp
import os
import random
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

# ================= é…ç½®å€åŸŸ =================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
TEST_IMAGE_DIR = './data/test_images/'
MODEL_WEIGHTS = "best_mit_b5_scse_model.pth"

# é¡è‰²å®šç¾© (RGB)
COLOR_MAP = {
    0: (0, 0, 0),       # Background
    1: (255, 0, 0),     # Aggregation (Red)
    2: (0, 255, 0),     # Normal (Green)
    3: (255, 255, 0),   # Blur (Yellow)
    4: (128, 0, 128),   # Abnormal (Purple)
    5: (0, 255, 255)    # Hemo (Cyan)
}

CLASSES = {
    0: "Background",
    1: "Aggregation",
    2: "Normal",
    3: "Blur",
    4: "Abnormal",
    5: "Hemo"
}

# ================= æ ¸å¿ƒï¼šå…ˆé€²å¾Œè™•ç†ç®—æ³• =================

def smart_post_processing(raw_mask):
    """
    å…ˆé€²å¾Œè™•ç†æµç¨‹ï¼š
    1. å™ªé»éæ¿¾
    2. å¹¾ä½•æ‹¼æ¥ (ä¿®å¾©æ–·å±¤ï¼Œä¸åˆä½µé„°å±…)
    3. å¤šæ•¸æ±ºæŠ•ç¥¨ (çµ±ä¸€é¡è‰²)
    """
    h, w = raw_mask.shape
    
    # 1. åˆå§‹åŒ–ä¸€å€‹å…¨é»‘çš„ç•«å¸ƒ
    final_mask = np.zeros_like(raw_mask)
    
    # 2. å»ºç«‹äºŒå€¼åŒ– Mask (åªè¦ä¸æ˜¯èƒŒæ™¯éƒ½ç®—å‰æ™¯)
    # ç”¨ä¾†æ‰¾å‡ºæ‰€æœ‰çš„è¡€ç®¡ç¢ç‰‡ï¼Œä¸ç®¡å®ƒæ˜¯ä»€éº¼é¡è‰²
    binary_mask = (raw_mask > 0).astype(np.uint8) * 255
    
    # 3. åŸºç¤å»å™ª (å»é™¤æ¥µå°é›œé»)
    kernel_clean = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel_clean)
    
    # 4. æ‰¾å‡ºæ‰€æœ‰ç¨ç«‹çš„é€£é€šå€åŸŸ (Component Analysis)
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, connectivity=8)
    
    # å„²å­˜æœ‰æ•ˆçš„è¡€ç®¡ç¢ç‰‡ä¿¡æ¯
    # æ ¼å¼: {'id': label_id, 'x': center_x, 'y_bottom': y+h, 'y_top': y, 'bbox': ...}
    fragments = []
    
    min_area = 30 # æœ€å°é¢ç©é–¾å€¼
    
    for i in range(1, num_labels): # è·³éèƒŒæ™¯ 0
        area = stats[i, cv2.CC_STAT_AREA]
        if area < min_area:
            continue
            
        x, y, w_rect, h_rect = stats[i, cv2.CC_STAT_LEFT], stats[i, cv2.CC_STAT_TOP], stats[i, cv2.CC_STAT_WIDTH], stats[i, cv2.CC_STAT_HEIGHT]
        center_x = centroids[i][0]
        
        fragments.append({
            'id': i,
            'center_x': center_x,
            'y_top': y,
            'y_bottom': y + h_rect,
            'area': area,
            'merged': False, # æ¨™è¨˜æ˜¯å¦å·²è¢«åˆä½µ
            'group_id': i    # åˆå§‹æ™‚è‡ªå·±ä¸€çµ„
        })

    # 5. ã€é—œéµæŠ€è¡“ã€‘å¹¾ä½•æ‹¼æ¥ (Geometric Stitching)
    # é‚è¼¯ï¼šå¦‚æœåœ¨ X è»¸ä¸Šå¾ˆæ¥è¿‘ï¼Œä¸”åœ¨ Y è»¸ä¸Šæ–·è£‚è™•å¾ˆè¿‘ï¼Œå‰‡è¦–ç‚ºåŒä¸€æ¢
    
    # åƒæ•¸è¨­å®š
    MAX_X_DIST = 10   # X è»¸å…è¨±çš„æœ€å¤§åå·® (åƒç´ )
    MAX_Y_GAP = 50    # Y è»¸å…è¨±çš„æœ€å¤§æ–·è£‚è·é›¢ (åƒç´ )
    
    # ä¾æ“š Y è»¸ä½ç½®æ’åºï¼Œå¾ä¸Šåˆ°ä¸‹è™•ç†
    fragments.sort(key=lambda k: k['y_top'])
    
    # å»ºç«‹åˆä½µæ˜ å°„è¡¨ (Union-Find æ¦‚å¿µçš„ç°¡åŒ–ç‰ˆ)
    merge_map = {f['id']: f['id'] for f in fragments}
    
    for i in range(len(fragments)):
        f1 = fragments[i]
        
        # å¾€å¾Œæ‰¾å¯èƒ½çš„æ‹¼æ¥å°è±¡
        for j in range(i + 1, len(fragments)):
            f2 = fragments[j]
            
            # å¦‚æœ f2 çš„é ‚éƒ¨å·²ç¶“é›¢ f1 åº•éƒ¨å¤ªé ï¼Œå¾Œé¢çš„æ›´ä¸ç”¨çœ‹äº† (å› ç‚ºå·²ç¶“æŒ‰ Y æ’åº)
            if f2['y_top'] - f1['y_bottom'] > MAX_Y_GAP:
                break
                
            # æª¢æŸ¥ X è»¸å°é½Šç¨‹åº¦ (æ˜¯å¦åœ¨åŒä¸€å‚ç›´ç·šä¸Š)
            if abs(f1['center_x'] - f2['center_x']) < MAX_X_DIST:
                # æ‰¾åˆ°åŒ¹é…ï¼åˆä½µå®ƒå€‘
                # å°‡ f2 çš„çµ„åˆ¥è¨­ç‚º f1 çš„çµ„åˆ¥
                root_group = merge_map[f1['id']]
                merge_map[f2['id']] = root_group
                
                # æ›´æ–° f1 çš„åº•éƒ¨ä½ç½®ï¼Œä»¥ä¾¿èƒ½ç¹¼çºŒå¾€ä¸‹æ¥æ›´ä¸‹é¢çš„ç¢ç‰‡
                f1['y_bottom'] = max(f1['y_bottom'], f2['y_bottom'])

    # 6. ã€é—œéµæŠ€è¡“ã€‘å¤šæ•¸æ±ºæŠ•ç¥¨ (Majority Voting)
    # æ ¹æ“šåˆä½µå¾Œçš„çµ„åˆ¥ï¼Œé‡æ–°ç¹ªè£½ Mask
    
    # å°‡ fragment ä¾æ“š group_id åˆ†çµ„
    groups = {}
    for f in fragments:
        gid = merge_map[f['id']]
        if gid not in groups:
            groups[gid] = []
        groups[gid].append(f['id'])
        
    final_stats = {}
    
    for gid, member_ids in groups.items():
        # å‰µå»ºé€™å€‹çµ„åˆ¥çš„ Mask
        group_mask = np.isin(labels, member_ids)
        
        # åœ¨åŸå§‹é æ¸¬åœ–ä¸­ï¼Œæ‰¾å‡ºé€™å€‹å€åŸŸæ¶µè“‹çš„æ‰€æœ‰åƒç´ é¡åˆ¥
        # raw_mask[group_mask] æœƒå–å‡ºè©²å€åŸŸæ‰€æœ‰åƒç´ çš„é¡åˆ¥å€¼
        pixel_values = raw_mask[group_mask]
        
        # éæ¿¾æ‰ 0 (èƒŒæ™¯)ï¼Œé›–ç„¶ç†è«–ä¸Š binary mask å·²ç¶“éæ¿¾äº†ï¼Œä¿éšªèµ·è¦‹
        pixel_values = pixel_values[pixel_values > 0]
        
        if len(pixel_values) == 0:
            continue
            
        # çµ±è¨ˆå‡ºç¾æœ€å¤šæ¬¡çš„é¡åˆ¥ (Mode)
        counts = np.bincount(pixel_values)
        dominant_class = np.argmax(counts)
        
        # ç¹ªè£½åˆ°æœ€çµ‚ Mask ä¸Šï¼š
        # é€™è£¡æœ‰å…©ç¨®ç•«æ³•ï¼š
        # A. åªç•«åŸæœ¬çš„ç¢ç‰‡ (ä¿æŒæ–·è£‚ä½†é¡è‰²çµ±ä¸€)
        # B. ç”¨ç·šæŠŠç¢ç‰‡é€£èµ·ä¾† (ä¿®å¾©æ–·è£‚) -> æˆ‘å€‘é¸ B
        
        # æ‰¾å‡ºè©²çµ„åˆ¥æ‰€æœ‰ç¢ç‰‡çš„è¼ªå»“ä¸¦ç•«ä¸Šå»
        # ç‚ºäº†é€£æ¥æ–·è£‚è™•ï¼Œæˆ‘å€‘è¨ˆç®—é€™äº›ç¢ç‰‡çš„ Convex Hull (å‡¸åŒ…) æˆ–è€…ç›´æ¥ç•«ç·š
        # ç°¡å–®åšæ³•ï¼šåˆ†åˆ¥ç•«å‡ºæ¯å€‹ç¢ç‰‡ï¼Œç„¶å¾Œå¦‚æœæ˜¯åŒä¸€çµ„ï¼Œç•«ä¸€æ¢ç·šé€£æ¥å®ƒå€‘çš„é‡å¿ƒ
        
        # å…ˆç•«åŸå§‹ç¢ç‰‡ï¼Œçµ±ä¸€é¡è‰²
        final_mask[group_mask] = dominant_class
        
        # å†ç•«é€£æ¥ç·š (ä¿®å¾©è¦–è¦ºæ–·å±¤)
        if len(member_ids) > 1:
            member_frags = [f for f in fragments if f['id'] in member_ids]
            member_frags.sort(key=lambda k: k['y_top'])
            for k in range(len(member_frags) - 1):
                pt1 = (int(member_frags[k]['center_x']), int(member_frags[k]['y_bottom']))
                pt2 = (int(member_frags[k+1]['center_x']), int(member_frags[k+1]['y_top']))
                # ç•«ä¸€æ¢ç²—ç·šé€£æ¥
                cv2.line(final_mask, pt1, pt2, int(dominant_class), thickness=5)

        # çµ±è¨ˆæœ€çµ‚æ•¸é‡
        class_name = CLASSES.get(dominant_class, "Unknown")
        final_stats[class_name] = final_stats.get(class_name, 0) + 1

    return final_stats, final_mask

# ================= æ¨¡å‹è¼‰å…¥èˆ‡è¼”åŠ©å‡½å¼ =================
def mask_to_rgb(mask, color_map):
    h, w = mask.shape
    rgb = np.zeros((h, w, 3), dtype=np.uint8)
    for class_id, color in color_map.items():
        rgb[mask == class_id] = color
    return rgb

def load_model():
    print(f"ğŸš€ Loading Model: MiT-B5 (scSE)...")
    model = smp.Unet(
        encoder_name="mit_b5",
        classes=6,
        decoder_attention_type="scse"
    ).to(DEVICE)
    
    if os.path.exists(MODEL_WEIGHTS):
        state_dict = torch.load(MODEL_WEIGHTS, map_location=DEVICE)
        if 'model_state_dict' in state_dict:
            state_dict = state_dict['model_state_dict']
        model.load_state_dict(state_dict, strict=True)
        model.eval()
        return model
    else:
        raise FileNotFoundError(f"Model weights not found: {MODEL_WEIGHTS}")

def run_pipeline():
    # 1. Setup
    model = load_model()
    img_files = [f for f in os.listdir(TEST_IMAGE_DIR) if f.endswith(('.jpg', '.png'))]
    if not img_files: raise FileNotFoundError("No images in test folder")
    
    random_file = random.choice(img_files)
    # random_file = "8_54890_5.jpg" # å¦‚æœè¦å›ºå®šæ¸¬è©¦æŸå¼µåœ–
    
    img_path = os.path.join(TEST_IMAGE_DIR, random_file)
    print(f"ğŸ” Analyzing Image: {random_file}")
    
    # 2. Preprocessing
    image_bgr = cv2.imread(img_path)
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    h, w = image_rgb.shape[:2]
    
    transform = A.Compose([
        A.Resize(640, 640),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2(),
    ])
    
    input_tensor = transform(image=image_rgb)['image'].unsqueeze(0).to(DEVICE)
    
    # 3. Inference
    with torch.no_grad():
        logits = model(input_tensor)
        pred_raw = torch.argmax(logits, dim=1).squeeze().cpu().numpy().astype(np.uint8)
        pred_mask = cv2.resize(pred_raw, (w, h), interpolation=cv2.INTER_NEAREST)

    # 4. Advanced Post-Processing
    stats, refined_mask = smart_post_processing(pred_mask)
    
    # 5. Visualization
    visualize_results(image_rgb, pred_mask, refined_mask, stats)

def visualize_results(original, raw_mask, refined_mask, stats):
    fig, axes = plt.subplots(1, 3, figsize=(20, 8))
    
    # Original
    axes[0].imshow(original)
    axes[0].set_title("Original Image", fontsize=14)
    axes[0].axis('off')
    
    # Raw Prediction
    raw_rgb = mask_to_rgb(raw_mask, COLOR_MAP)
    axes[1].imshow(raw_rgb)
    axes[1].set_title("Raw Prediction\n(Mixed Colors & Fragments)", fontsize=14)
    axes[1].axis('off')
    
    # Refined Prediction
    refined_rgb = mask_to_rgb(refined_mask, COLOR_MAP)
    axes[2].imshow(refined_rgb)
    
    # Generate Stat Text
    stats_text = "Advanced Analysis Result:\n"
    for k, v in stats.items():
        stats_text += f"{k}: {v}\n"
        
    axes[2].set_title(stats_text, fontsize=12, loc='left', family='monospace', fontweight='bold')
    axes[2].axis('off')
    
    # Legend
    patches = []
    for cls_id, color in COLOR_MAP.items():
        if cls_id == 0: continue
        c_norm = (color[0]/255, color[1]/255, color[2]/255)
        patches.append(mpatches.Patch(color=c_norm, label=CLASSES[cls_id]))
    
    fig.legend(handles=patches, loc='lower center', ncol=6, fontsize='large')
    
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.1)
    save_path = "advanced_result.png"
    plt.savefig(save_path)
    print(f"âœ… Visualization Saved: {save_path}")
    plt.show()

if __name__ == "__main__":
    run_pipeline()